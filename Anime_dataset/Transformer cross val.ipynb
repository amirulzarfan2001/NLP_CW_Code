{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_import import *\n",
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_val = pd.read_csv('df_val.csv')\n",
    "df_test = pd.read_csv('df_test.csv')\n",
    "df_full = pd.read_csv('final_cleaned_dataset_df.csv')\n",
    "\n",
    "#Make sure the genre collumns is in lists not strings\n",
    "#NEED TO DO THIS EVERYTIME EXPORT DATASET\n",
    "df_train['genres'] = df_train['genres'].apply(lambda x: list(ast.literal_eval(x)))\n",
    "df_val['genres'] = df_val['genres'].apply(lambda x: list(ast.literal_eval(x)))\n",
    "df_test['genres'] = df_test['genres'].apply(lambda x: list(ast.literal_eval(x)))\n",
    "\n",
    "df_train=df_train.drop(columns=['title','index'])\n",
    "df_val=df_val.drop(columns=['title','index'])\n",
    "df_test=df_test.drop(columns=['title','index'])\n",
    "\n",
    "\n",
    "display(df_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ========== STEP 1: Setup & Data Prep ==========\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    f1_score, jaccard_score, hamming_loss, accuracy_score,precision_score, recall_score\n",
    ")\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "# Confirm GPU availability\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "# ========== STEP 2: Label Setup ==========\n",
    "# Assuming df_train, df_val, df_test exist and contain \"synopsis\" and \"genres\" columns\n",
    "all_genres = sorted(set(genre for sublist in df_train[\"genres\"] for genre in sublist))\n",
    "label2id = {genre: idx for idx, genre in enumerate(all_genres)}\n",
    "id2label = {idx: genre for genre, idx in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "def encode_labels(genres):\n",
    "    vec = np.zeros(num_labels, dtype=np.float32)\n",
    "    for genre in genres:\n",
    "        vec[label2id[genre]] = 1.0\n",
    "    return vec\n",
    "\n",
    "df_train[\"labels\"] = df_train[\"genres\"].apply(encode_labels)\n",
    "df_val[\"labels\"] = df_val[\"genres\"].apply(encode_labels)\n",
    "df_test[\"labels\"] = df_test[\"genres\"].apply(encode_labels)\n",
    "display(df_train)\n",
    "\n",
    "\n",
    "\n",
    "# ========== STEP 3: Tokenization ==========\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "train_encodings = tokenizer(df_train[\"synopsis\"].tolist(), truncation=True)\n",
    "val_encodings = tokenizer(df_val[\"synopsis\"].tolist(), truncation=True)\n",
    "test_encodings = tokenizer(df_test[\"synopsis\"].tolist(), truncation=True)\n",
    "\n",
    "# ========== STEP 4: Create Datasets ==========\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "    \"labels\": list(df_train[\"labels\"])\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": val_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": val_encodings[\"attention_mask\"],\n",
    "    \"labels\": list(df_val[\"labels\"])\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": test_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "    \"labels\": list(df_test[\"labels\"])\n",
    "})\n",
    "\n",
    "# ========== STEP 5: Model Setup ==========\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_ckpt,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config)\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== STEP 6: Custom Trainer ==========\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # <--- added **kwargs here\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = BCEWithLogitsLoss()(logits, labels.float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# ========== STEP 7: Metrics ==========\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    probability = sigmoid(pred.predictions)  # fixed typo: pred.predications â†’ pred.predictions\n",
    "    preds = (probability > 0.5).astype(int)\n",
    "\n",
    "    f1 = f1_score(labels, preds, average=\"samples\")\n",
    "    precision = precision_score(labels, preds, average=\"samples\")\n",
    "    recall = recall_score(labels, preds, average=\"samples\")\n",
    "    jaccard = jaccard_score(labels, preds, average=\"samples\")\n",
    "    hits = (np.logical_and(labels, preds).sum(axis=1) > 0).mean()\n",
    "    hamming = hamming_loss(labels, preds)\n",
    "    exact = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"f1_samples\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"jaccard\": jaccard,\n",
    "        \"hit_rate\": hits,\n",
    "        \"hamming_loss\": hamming,\n",
    "        \"exact_match\": exact\n",
    "    }\n",
    "\n",
    "# ========== STEP 8: Data Collator ==========\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "# ========== STEP 9: Training Arguments ==========\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./7k_distilbert-base-uncased_batch12_LRlindecay0.1\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"linear\",         # decay strategy: linear, cosine, polynomial, etc.\n",
    "    warmup_ratio=0.1,   \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  # Save based on validation loss\n",
    "    greater_is_better=False,            # Lower loss is better\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    log_level=\"info\"\n",
    ")\n",
    "\n",
    "# ========== STEP 10: Trainer ==========\n",
    "trainer = MultiLabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# ========== STEP 11: Train ==========\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
