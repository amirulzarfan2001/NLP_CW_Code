{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545ff07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "synopsis",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "genres",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "874591e4-1510-4b6b-9c69-5155e1637ff5",
       "rows": [
        [
         "0",
         "Sometime in the future, the world was completely dried up and became all desert. They had little rivers and lakes left, which villians and dangerous animals lived. Water has become the most valueable thing on the world. Whoever can control water will rule over the world.  (Source: ANN)",
         "['Action', 'Adventure']"
        ],
        [
         "1",
         "Set in 2014, the anime follows the adventures of 23 years old Mafuneko, a newly minted assistant director who joins the TV production department at Tokyo Hajikko Television, only to discover that the glamorous and glitzy life of working behind-the-scenes making TV shows involves strange and inexplicable tasks such as \"gathering 300 acorns\" and \"making a mosaic out of the images reflected in the camera lens.\" Despite being surrounded by chaos, set-backs, and weirdos, Mafuneko struggles to become a fully-fledged TV producer.  (Source: Crunchyroll, edited)",
         "['Comedy']"
        ],
        [
         "2",
         "Follows a pig whose family's mission is to collect underpants.",
         "['Comedy', 'Kids']"
        ],
        [
         "3",
         "In honor of the 2018 World Cup, this season of GG Bond focuses on sports team competition, team spirit, and personal growth as they compete in the Meteor League.  ",
         "['Kids', 'Sci-Fi', 'Sports', 'Super Power']"
        ],
        [
         "4",
         "Fairies living in a fluffy forest, where both flowers and trees are fluffy. Follow a witch's trouble-maker apprentices Pui Pui and Muu Muu.",
         "['Fantasy', 'Kids']"
        ],
        [
         "5",
         "This story is about Mick, a sleeping chironomid. Insects are taken up into space for use in experiments by humans. An epic space adventure of Mick and his friends of their journey back home to Earth. The development plans and experiments are aborted by humans for no known reason, the organisms are left behind in the space station with limited resources. Going over hurdles and fighting through challenges, together, they travel across planets and gain new comrades, heading for Earth. As their journey progresses, the mystery behind the human’s abandonment unravels…  (Source: Crunchyroll)",
         "['Adventure', 'Other']"
        ],
        [
         "6",
         "The anime is based on MegaHouse's line of Ziguru Hazeru (literally, \"Jiggle and Pop Out\") mecha toys that allow people to remove parts from the joints and add different parts in their place. In addition, if one combines together Hazeru Gokuu, Hazeru Sagojo, and Hazeru Hakkai, the resulting mecha will be Hazeru Seioh, allowing for users to combine their toys together.  The ZIGURUHAZERU animated series is based on the upcoming original toy line of the same name from MegaHouse, featuring cool robot toys modelled after characters from the famous Journey to the West tale.",
         "['Action', 'Mecha']"
        ],
        [
         "7",
         "Fuwa Beibei encounters the severely injured goddess, Athena. In order to help Athena regain strength, Beibei sets out to find a mysterious wooden box left by Coubertin in 1894, but the worrying thing is that Pixiu who injured his stomach from his glutinous eating habits heard this news too. He thought that this mysterious energy could not only cure his stomach, but also improve his magic, and such begins the journey through time and space of two spirits with different aims. After all the hardships, they found the sixth son of the dragon - Bixi, and after the two's efforts in passing his test, Bixi carries them along 100 years of Olympic history.",
         "['Historical']"
        ],
        [
         "8",
         "The anime will depict various vehicles as characters in the \"Mobile Land\" island. The protagonist Car-kun arrives at the island as a delivery worker. Through his deliveries, he gets to know the residents of Mobile Land, all while following traffic rules and having his deliveries stopped by the meddling Sabibi.  (Source: ANN)",
         "['Comedy', 'Kids', 'Other', 'Slice of Life']"
        ],
        [
         "9",
         "follows the adventures of an adorable jet plane named Jett who travels around the world delivering packages to children. On every delivery, Jett encounters new problems that he must solve with the help of his friends the \"Super Wings\": Dizzy, Jerome and Donnie. Together, the team explores different countries and learns the diversity of new cultures.  (Source: Official YouTube channel)",
         "['Adventure', 'Kids', 'Other']"
        ],
        [
         "10",
         "A collaboration shorts that aired as part of Yomiuri TV's morning program block.",
         "['Comedy']"
        ],
        [
         "11",
         "There was once a man who was summoned to another world, and saved it. Of course, he became too popular there, and turned into an isekai-normie. However, that man fell into a \"trap\" and was forcibly returned to his original world. Moreover, he had to start over as a baby!  This is the story of the way-too-fantastic ex-hero who lived as a gloomy high-schooler, as he gets summoned once again to that same other world in a very unexpected development!  (Source: Coolmic, edited)",
         "['Action', 'Adventure', 'Comedy', 'Fantasy', 'Harem', 'Romance']"
        ],
        [
         "12",
         "The 2018 LINE sticker set Poccolies is inspiring a series of anime shorts. The story of the sticker set is that a positive and honest boy named Patsuhiko lives on Pokkori Island, along with the shrewd and mysterious Ham, and a reliable older brother figure named Kangaroo. The sticker set follows their daily lives.  (Source: ANN)",
         "['Kids', 'Slice of Life']"
        ],
        [
         "13",
         "Fourth season of Kamiusagi Rope, events occur after the feature film. Airs on Mezamashi TV instead of the theaters.",
         "['Comedy', 'Slice of Life']"
        ],
        [
         "14",
         "Korean Animation about the monsters Eerie, Rock-G, Popo, and Yossi and the adventures they go on each day!",
         "['Adventure']"
        ],
        [
         "15",
         "TV adaptation of Cocone Corporation's mobile puzzle game app.  Discover the adventures of the cat painter, Vincent van Meowogh who lives in a mansion in Paris surrounded by his quirky friends. Let's give him a spark of inspiration for his painting by completing puzzle stages, buy back the furniture and restore his home to its former glory!  (Source: ANN)",
         "['Kids', 'Other']"
        ],
        [
         "16",
         "General of the Three Kingdoms, Kongming had struggled his whole life, facing countless battles that made him into the accomplished strategist he was. So on his deathbed, he wished only to be reborn into a peaceful world... and was sent straight to modern-day party-central, Tokyo! Can even a brilliant strategist like Kongming adapt to the wild beats and even wilder party people?!  (Source: Kodansha US, edited)",
         "['Comedy']"
        ],
        [
         "17",
         "Kou Yamori seems like a typical middle school student on the surface. Relatively good at studies and amiable with his classmates, he puts a lot of effort into maintaining this facade. One day, however, he decides to stop pretending and quits school, developing insomnia as a result of having no daytime outlet for his energy. When taking walks alone at night, he feels marginally better, though he is aware that his inability to sleep should be considered a serious problem.  On one such walk, Kou meets a weird girl, Nazuna Nanakusa, who diagnoses the cause of his sleeplessness: despite making changes in his life, he is still holding himself back from experiencing true freedom. She says that he won't be able to sleep unless he is satisfied with how he spends his waking hours. When it appears that she has resolved his current worries, Nazuna invites him back to her apartment to share her futon. After a while, unaware that he is only feigning unconsciousness, she leans over him—and bites his neck!  ",
         "['Other', 'Romance', 'Shounen', 'Supernatural']"
        ],
        [
         "18",
         "Ryou Mizushima enrolls in junior high and joins the badminton club with great vigor. Although the club didn't have a proper coach, Mizushima improved his skills with his own physical strength by the time he participated in the prefectural tournament. Afterwards, he is approached by the Yokohama Minato High School's badminton coach, Ebihara.  Indecisive and timid, Mizushima hesitates to attend such a prestigious school. However, with his elder sister Rika giving him a strong push, he decides to go to Yokohama Minato. Now blessed with a coach and idiosyncratic teammates, Mizushima will gain experience he's lacked till now, and aims to win the inter-high tournament.  (Source: MAL News)",
         "['Sports']"
        ],
        [
         "19",
         "This show consists of shorts starring 7 characters created and voiced by the voice actors of the variety show \"Seiyuu Danshi desu ga...?\". The anime will be a slapstick comedy that follows seven spirits that come to the modern world.  (Source: ANN)",
         "['Comedy', 'Supernatural']"
        ],
        [
         "20",
         "Hello Jadoo is a South-Korean TV show about a girl named Choi Jadoo who is a very free-spirited girl. In every episode, she would always get into drama, whether it was with her parents or school. At the same time, she meets new people and she tries to solve problems occurring.",
         "['Slice of Life']"
        ],
        [
         "21",
         "The supreme Gods who had too much free time created the ultimate brain games \"Play of the Gods.\" Former Goddess Leche awoke from a long slumber and declared to the world, \"Bring forth the person who is the best in games in this era!\" Fay is nominated to represent humanity as the \"best rookie in recent years.\"  The \"Game of the Gods\" that is about to begin between the two may be a little too difficult, as there has yet to be a victor throughout human history, because Gods are capricious, very unreasonable, and sometimes completely incomprehensible. However, given the nature of the games, it would be a waste not to have a good time and play with all of one's heart! The ultimate brain battles of a genius gamer boy, a former Goddess, and friends begin!  (Source: MAL News)",
         "['Ecchi', 'Fantasy', 'Other']"
        ],
        [
         "22",
         "Struggling with life and society, high school student Michio Kaga wanders about the Internet and lands on an odd website. The website, featuring a number of questions and a point based system, allows one to create skills and abilities for a character. Upon completing his character, Kaga was transported to a game-like fantasy world and reborn as a strong man who can claim idol-level girls. Thus begins the cheat and harem legend of a reborn man!  (Source: MAL News)",
         "['Action', 'Adventure', 'Fantasy', 'Harem', 'Romance']"
        ],
        [
         "23",
         "Technoroid: Overmind is set in the Entertainment Tower Babel, a new hope found by those who have lost the joy of light as humanity's activities are restricted due to the large-scale climate change caused by the expanded sun. The anime depicts unique units and characters fighting for the top of Babel, pursuing emotions that move people and androids through performances.  (Source: MAL News)",
         "['Music']"
        ],
        [
         "24",
         "Tsukumogami—spirits or \"marebito\" can possess objects of considerable age and gain a physical form. Although he is part of the Saenome clan that is in charge of peacefully sending them back to their own world, Hyouma Kunato despises them because one took away what was very precious to him. In order to cure him of this loathing, Hyouma's grandfather sends him to live with Botan Nagatsuki, a girl who is the master of six \"friendly\" tsukumogami and lives with them as a family.  (Source: MU, edited)",
         "['Action', 'Seinen', 'Supernatural']"
        ],
        [
         "25",
         "A young woman aims to become a famous fashion designer.  (Source: AniDB)",
         "['Romance', 'Shoujo']"
        ],
        [
         "26",
         "The hero of Tensei shitara Ken deshita differs from your standard otherworldly protagonist in that he is reincarnated as a sword! Beginning his quest by spawning in the middle of a beast-ridden forest, he encounters an injured girl frantically fleeing for her life. Saving her from her assailants, the pair acquaint themselves, and the girl introduces herself as Fran. She bears a heavy past, having endured the enslavement and maltreatment of her tribe, the Black Cats.  As the hero is unable to remember the name from his past life, the young and tenacious Fran bestows him the name \"Shishou\" and becomes his wielder. Thereafter, Shishou and Fran become a formidable team, embarking on quests to liberate the oppressed and exact justice!  ",
         "['Action', 'Fantasy']"
        ],
        [
         "27",
         "If a girl teases you, that means she likes you!  Unfortunately, Akiteru knows from experience that isn't the case. Because every girl he interacts with shows him nothing but scorn, and he's not scored a single date from it! Luckily, he's more concerned with securing a spot for him and his game-development buddies at his uncle's business.  But when his uncle throws him a condition that involves playing the part of his daughter's boyfriend, Akiteru has no choice but to take it. What will his best friend's sister Iroha, who bullies him relentlessly, think of the news?  (Source: J-Novel Club)",
         "['Comedy', 'Romance', 'School']"
        ],
        [
         "28",
         "There is a mysterious power of the Holy Spirit hidden in the legendary continent of Labelle, and the way to get power is recorded in the book of flower, guard by numerous flower Fairy Kings,only the legendary magic ambassador is recognized to wake up the power, and use them to defeat evil. For this.The evil force,led by Jakarta,the Queen of Dark had start a war to the continent of Labelle.With the help of the Flower Goddess,Pupula and the tribe of gouring flower fairys, the fate of the continent of Labelle is linked with an Earth girl... Who is the legendary envoy XiaAnAn.  (Source: Magical Girl Wiki)",
         "['Kids']"
        ],
        [
         "29",
         "Denji has a simple dream—to live a happy and peaceful life, spending time with a girl he likes. This is a far cry from reality, however, as Denji is forced by the yakuza into killing devils in order to pay off his crushing debts. Using his pet devil Pochita as a weapon, he is ready to do anything for a bit of cash.  Unfortunately, he has outlived his usefulness and is murdered by a devil in contract with the yakuza. However, in an unexpected turn of events, Pochita merges with Denji's dead body and grants him the powers of a chainsaw devil. Now able to transform parts of his body into chainsaws, a revived Denji uses his new abilities to quickly and brutally dispatch his enemies. Catching the eye of the official devil hunters who arrive at the scene, he is offered work at the Public Safety Bureau as one of them. Now with the means to face even the toughest of enemies, Denji will stop at nothing to achieve his simple teenage dreams.  ",
         "['Action', 'Adventure', 'Demons', 'Shounen']"
        ],
        [
         "30",
         "6.5-centimeter tall Degirdians fall to Earth in their spacecraft while chasing fugitives. Their spaceship transforms into a capsule toy stand near a toy store and a human boy accidentally takes home one of the aliens thinking it's a capsule toy. They befriend a human boy and try to figure out a way to get home all while fighting off the evil alien.",
         "['Kids', 'Sci-Fi']"
        ],
        [
         "31",
         "revolves around mischievous creatures called Teenieping that like to enter people's minds, but their playful nature and magical powers can wreak havoc in their hosts' lives. When the Teenieping are set loose on Earth, Princess Romi of the Emotion Kingdom becomes an ordinary girl that has to turn into a magical girl to stop them, all while balancing her new civilian life as a worker for the Bakery Heartrose with her coworkers Ian, Kyle, and Jun.  (Source: Wikipedia)",
         "['Fantasy', 'Kids', 'School']"
        ],
        [
         "32",
         "A more educational season of GG Bond. This season is composed of 2 arcs, each arc has 52 episodes.  GG Bond works on cleaning up some fossils at the lab as they'll be used in a museum exhibit. He uses a fossil to create a data coin that is compatible with his special jet. This allows for him to travel back in time to the original animal (a T-Rex) the fossil was made of. While GG Bond is ecstatic to learn about dinosaurs in-person (as well as mine more accurate data for the data coin), he didn't quite realize the danger he'd be in. He successfully gets out of this sticky situation but there's still a lot more dinosaurs he needs to get information on.",
         "['Historical', 'Kids', 'Sci-Fi', 'Super Power']"
        ],
        [
         "33",
         "Hitori Gotou is a high school girl who's starting to learn to play the guitar because she dreams of being in a band, but she's so shy that she hasn't made a single friend. However, her dream might come true after she meets Nijika Ijichi, a girl who plays drums and is looking for a new guitarist for her band.  (Source: MU, edited)",
         "['Comedy', 'Music', 'Slice of Life']"
        ],
        [
         "34",
         "TV anime based on San-X's new series of mascot characters \"Chickip Dancers.\" The main characters in the anime will be the apprehensive but curious bone-in chicken Hone Chicken, and the dancing instructor frog Skip Gaeru, who travels by dancing.  (Source: ANN)",
         "['Slice of Life']"
        ],
        [
         "35",
         "Stop-motion animation using clay and puppets follows a musician and hit cat traveling the world and learning about music in different cultures while spreading their own songs.",
         "['Kids', 'Music']"
        ],
        [
         "36",
         "A Chinese prince meets a regular civilian during his travels and quickly falls in love. They marry and have a daughter, but the prince is soon separated from them during a rebellion. Many years later, the prince has become emperor and locates his lost daughter. She comes to live with him, but unfortunately has picked up several unrefined habits during her times as a civilian.",
         "['Comedy', 'Historical', 'Kids']"
        ],
        [
         "37",
         "is an animation series with some educational undertones targeting 5~7-year-old kids. The entire series is set against the backdrop of a supermarket. Viewing the animations, children will be able to imagine themselves in amidst all a supermarket has to offer. Children can join Kemy and his quest to figure out the answers to everyday questions. Buka and Chaka do not reprise their roles in this season.  (Source: korean-products.com)  ",
         "['Kids']"
        ],
        [
         "38",
         "Centers on a penguin who works in an abusive company and gripes about the difficulties of workplace life. The title is a play on the \"koutei penguin,\" the Japanese name for the emperor penguin.  (Source: ANN)",
         "['Other']"
        ],
        [
         "39",
         "No synopsis has been added for this series yet.   to update this information.",
         "['Comedy', 'Fantasy']"
        ],
        [
         "40",
         "In \"a certain era\" on \"a certain continent,\" banal things are happening. The construction of \"Large-Scale Leisure Facilities\" has demanded that all the villages are to be evicted. When these demands are denied, the village and its residents are obliterated. A boy named Chap once lived in tranquility, but that happiness soon came crumbling down. He decides to embark on a journey of revenge, but soon falls madly in love with the king's daughter, Princess Melora. In order to fulfill her selfish wishes, he gathers his friends in order to overcome the impossible.",
         "['Adventure', 'Comedy', 'Fantasy', 'Kids']"
        ],
        [
         "41",
         "The story centers around a cat who answers people's wishes and delivers happiness. (\"Tane\" and \"neko\" are the Japanese words for \"seed\" and \"cat,\" respectively.)  (Source: ANN)",
         "['Comedy']"
        ],
        [
         "42",
         "The series follows a young girl named Shina who hopes to become the world's best DJ, and is interested in new sounds. Shina explores a mysterious world filled with Otoppe, strange creatures capable of unique sounds.",
         "['Fantasy', 'Kids', 'Music']"
        ],
        [
         "43",
         "The second season of the franchise. It aired within the \"Fight Tension☆School\" variety program.",
         "['Comedy', 'Mecha']"
        ],
        [
         "44",
         "Shorts that aired within \"Fight Tension☆Depart,\" a variety program that originally aired the anime as one of its segments. Celeb-chan is a narcissistic school girl who believes that she is a celebrity because her family is incredibly wealthy. She tries to do typical rich people things such as starting her own heavy makeup line, becoming a singer, saving an endangered species, buying a teacup poodle, etc. The results are never how she expects it.",
         "['Comedy', 'Parody']"
        ],
        [
         "45",
         "How did Asia's most prominent entrepreneurs know when to grasp opportunities? And how did they overcome difficulties and attain their positions of power? This series uses the unconventional technique of blending actual documentary footage with CG animation to get a close-up look into the front lines and what lies behind the scenes of Asian business.  Our cartoon host is Shima Kosaku, a character from a popular Japanese business manga. He delves into the heart of the Asian economic market representing 4 billion people, to visit entrepreneurs who have survived turbulent times. Thai business magnate, Dhanin Chearavanont (Chairman of CP Group), and innovative Taiwanese semiconductor entrepreneur, Morris Chang (Chairman of TSMC), are among the many movers and shakers to be featured. Don't miss this opportunity to glimpse the dramatic lives of these media-shy business titans.  (Source: Official Site)",
         "['Drama', 'Seinen']"
        ],
        [
         "46",
         "The story centers on Matthew, a daydreaming worrywart of a rabbit. Matthew also has two friends: Keron, a pessimistic frog who is fond of good-luck charms; and Ham, a hamster of few words and who moves through life at his own pace. The story follows the three animals' everyday lives within a mysterious forest.  (Source: ANN)",
         "['Slice of Life']"
        ],
        [
         "47",
         "Excellent student Iwakura Mitsumi has always dreamt about leaving her small town, going to a prestigious university, and making positive change in the world. But she's so focused on reaching her goals that she's not prepared for the very different (and overwhelming) city life that awaits her in a Tokyo high school. Luckily, she makes fast friends with Shima Sousuke, a handsome classmate who's as laid-back as she is over-prepared. Can this naive country girl make it big in Tokyo with Sousuke by her side?  (Source: Seven Seas Entertainment)",
         "['School', 'Seinen', 'Slice of Life']"
        ],
        [
         "48",
         "The story will follow four girls who seek to use their songs to heal and soothe the people of their world. The tagline reads, \"Kindhearted, powerful songs heal the world. These are beautiful miracles sung by humanity.\"  (Source: MAL Source)",
         "['Music']"
        ],
        [
         "49",
         "It follows the fan favorite character Vandyne as his hidden secrets are revealed. Unlike the reboot, which exclusively uses 3DCG visuals, this series is like the earlier entries and is animated with 2D and 3D visuals.  The mini-series is part of a new promotional campaign from Sono Kong. The Korean toy manufacturer will launch four new mechanimals this month at retail.  (Source: toonbarn.com)",
         "['Adventure', 'Kids', 'Mecha', 'Other']"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 9194
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synopsis</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sometime in the future, the world was complete...</td>\n",
       "      <td>[Action, Adventure]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Set in 2014, the anime follows the adventures ...</td>\n",
       "      <td>[Comedy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Follows a pig whose family's mission is to col...</td>\n",
       "      <td>[Comedy, Kids]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In honor of the 2018 World Cup, this season of...</td>\n",
       "      <td>[Kids, Sci-Fi, Sports, Super Power]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fairies living in a fluffy forest, where both ...</td>\n",
       "      <td>[Fantasy, Kids]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9189</th>\n",
       "      <td>The Konohagakure Grand Sports Festival has beg...</td>\n",
       "      <td>[Action, Comedy, Fantasy, Other, Shounen, Sports]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9190</th>\n",
       "      <td>Special bundled with the Blu-ray/DVD volume of .</td>\n",
       "      <td>[Ecchi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9191</th>\n",
       "      <td>According to the official Hobby Japan website,...</td>\n",
       "      <td>[Comedy, Ecchi, Fantasy, Parody]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9192</th>\n",
       "      <td>A series of comedic shorts featuring chibi ver...</td>\n",
       "      <td>[Adventure, Comedy, Fantasy, Parody]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9193</th>\n",
       "      <td>These shorts, included in the Blu-ray and DVD ...</td>\n",
       "      <td>[Comedy, Seinen, Slice of Life]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               synopsis  \\\n",
       "0     Sometime in the future, the world was complete...   \n",
       "1     Set in 2014, the anime follows the adventures ...   \n",
       "2     Follows a pig whose family's mission is to col...   \n",
       "3     In honor of the 2018 World Cup, this season of...   \n",
       "4     Fairies living in a fluffy forest, where both ...   \n",
       "...                                                 ...   \n",
       "9189  The Konohagakure Grand Sports Festival has beg...   \n",
       "9190   Special bundled with the Blu-ray/DVD volume of .   \n",
       "9191  According to the official Hobby Japan website,...   \n",
       "9192  A series of comedic shorts featuring chibi ver...   \n",
       "9193  These shorts, included in the Blu-ray and DVD ...   \n",
       "\n",
       "                                                 genres  \n",
       "0                                   [Action, Adventure]  \n",
       "1                                              [Comedy]  \n",
       "2                                        [Comedy, Kids]  \n",
       "3                   [Kids, Sci-Fi, Sports, Super Power]  \n",
       "4                                       [Fantasy, Kids]  \n",
       "...                                                 ...  \n",
       "9189  [Action, Comedy, Fantasy, Other, Shounen, Sports]  \n",
       "9190                                            [Ecchi]  \n",
       "9191                   [Comedy, Ecchi, Fantasy, Parody]  \n",
       "9192               [Adventure, Comedy, Fantasy, Parody]  \n",
       "9193                    [Comedy, Seinen, Slice of Life]  \n",
       "\n",
       "[9194 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from my_import import *\n",
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_val = pd.read_csv('df_val.csv')\n",
    "df_test = pd.read_csv('df_test.csv')\n",
    "df_full = pd.read_csv('final_cleaned_dataset_df.csv')\n",
    "\n",
    "#Make sure the genre collumns is in lists not strings\n",
    "#NEED TO DO THIS EVERYTIME EXPORT DATASET\n",
    "df_train['genres'] = df_train['genres'].apply(lambda x: list(ast.literal_eval(x)))\n",
    "df_val['genres'] = df_val['genres'].apply(lambda x: list(ast.literal_eval(x)))\n",
    "df_test['genres'] = df_test['genres'].apply(lambda x: list(ast.literal_eval(x)))\n",
    "\n",
    "df_train=df_train.drop(columns=['title','index'])\n",
    "df_val=df_val.drop(columns=['title','index'])\n",
    "df_test=df_test.drop(columns=['title','index'])\n",
    "\n",
    "\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405c30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "Using device: NVIDIA GeForce GTX 1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19016\\301147687.py:105: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MultiLabelTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = MultiLabelTrainer(\n",
      "***** Running training *****\n",
      "  Num examples = 9,194\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6,130\n",
      "  Number of trainable parameters = 66,975,004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6130' max='6130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6130/6130 16:45, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Samples</th>\n",
       "      <th>Jaccard</th>\n",
       "      <th>Hit Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.253300</td>\n",
       "      <td>0.249846</td>\n",
       "      <td>0.234319</td>\n",
       "      <td>0.178680</td>\n",
       "      <td>0.422360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.236609</td>\n",
       "      <td>0.302998</td>\n",
       "      <td>0.233347</td>\n",
       "      <td>0.528838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./anime-genre-model\\checkpoint-3065\n",
      "Configuration saved in ./anime-genre-model\\checkpoint-3065\\config.json\n",
      "Model weights saved in ./anime-genre-model\\checkpoint-3065\\model.safetensors\n",
      "tokenizer config file saved in ./anime-genre-model\\checkpoint-3065\\tokenizer_config.json\n",
      "Special tokens file saved in ./anime-genre-model\\checkpoint-3065\\special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1127\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./anime-genre-model\\checkpoint-6130\n",
      "Configuration saved in ./anime-genre-model\\checkpoint-6130\\config.json\n",
      "Model weights saved in ./anime-genre-model\\checkpoint-6130\\model.safetensors\n",
      "tokenizer config file saved in ./anime-genre-model\\checkpoint-6130\\tokenizer_config.json\n",
      "Special tokens file saved in ./anime-genre-model\\checkpoint-6130\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./anime-genre-model\\checkpoint-6130 (score: 0.3029975250028488).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6130, training_loss=0.25619256471148516, metrics={'train_runtime': 1006.3924, 'train_samples_per_second': 18.271, 'train_steps_per_second': 6.091, 'total_flos': 1218469973139456.0, 'train_loss': 0.25619256471148516, 'epoch': 2.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========== STEP 1: Setup & Data Prep ==========\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score, jaccard_score\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.metrics import f1_score, jaccard_score, hamming_loss, accuracy_score\n",
    "\n",
    "# Confirm GPU availability\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "# Example: df_train and df_test should have columns \"synopsis\" and \"genres\" (list of strings)\n",
    "# df_train = pd.read_csv(\"your_train_data.csv\")\n",
    "# df_test = pd.read_csv(\"your_test_data.csv\")\n",
    "\n",
    "# ========== STEP 2: Label Setup ==========\n",
    "# Extract unique genres\n",
    "all_genres = sorted(set(genre for sublist in df_train[\"genres\"] for genre in sublist))\n",
    "label2id = {genre: idx for idx, genre in enumerate(all_genres)}\n",
    "id2label = {idx: genre for genre, idx in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "# One-hot encode labels\n",
    "def encode_labels(genres):\n",
    "    vec = np.zeros(num_labels, dtype=np.float32)\n",
    "    for genre in genres:\n",
    "        vec[label2id[genre]] = 1.0\n",
    "    return vec\n",
    "\n",
    "df_train[\"labels\"] = df_train[\"genres\"].apply(encode_labels)\n",
    "df_val[\"labels\"] = df_val[\"genres\"].apply(encode_labels)\n",
    "df_test[\"labels\"] = df_test[\"genres\"].apply(encode_labels)\n",
    "\n",
    "# ========== STEP 3: Tokenization ==========\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "train_encodings = tokenizer(df_train[\"synopsis\"].tolist(), padding=True, truncation=True, return_tensors=\"np\", max_length=200)\n",
    "val_encodings = tokenizer(df_val[\"synopsis\"].tolist(), padding=True, truncation=True, return_tensors=\"np\", max_length=256)\n",
    "test_encodings = tokenizer(df_test[\"synopsis\"].tolist(), padding=True, truncation=True, return_tensors=\"np\", max_length=256)\n",
    "\n",
    "# ========== STEP 4: Create Datasets ==========\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "    \"labels\": list(df_train[\"labels\"])\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": val_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": val_encodings[\"attention_mask\"],\n",
    "    \"labels\": list(df_val[\"labels\"])\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": test_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "    \"labels\": list(df_test[\"labels\"])\n",
    "})\n",
    "\n",
    "# ========== STEP 5: Model Setup ==========\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_ckpt,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# ========== STEP 6: Custom Trainer ==========\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = BCEWithLogitsLoss()(logits, labels.float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ========== STEP 7: Metrics ==========\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = (pred.predictions > 0.5).astype(int)\n",
    "    f1 = f1_score(labels, preds, average=\"samples\")\n",
    "    jaccard = jaccard_score(labels, preds, average=\"samples\")\n",
    "    hits = (np.logical_and(labels, preds).sum(axis=1) > 0).mean()\n",
    "    return {\"f1_samples\": f1, \"jaccard\": jaccard, \"hit_rate\": hits}\n",
    "\n",
    "# ========== STEP 8: TrainingArgs ==========\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./anime-genre-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    log_level=\"info\",\n",
    "   \n",
    ")\n",
    "\n",
    "# ========== STEP 9: Trainer & Train ==========\n",
    "trainer = MultiLabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf61730",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train_encodings['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec10317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1139\n",
      "  Batch size = 3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-1.8221236 , -3.1561813 , -1.5802286 , ..., -5.560172  ,\n",
       "        -4.028331  , -1.2129418 ],\n",
       "       [-4.501064  , -3.9590275 ,  0.84210795, ..., -3.5361032 ,\n",
       "        -5.0380325 , -3.8415973 ],\n",
       "       [ 0.6901433 ,  0.01577828, -2.5583565 , ..., -4.143129  ,\n",
       "        -3.4288852 , -3.1410546 ],\n",
       "       ...,\n",
       "       [-4.694147  , -4.54762   , -0.9086509 , ..., -3.0604844 ,\n",
       "        -5.1731577 , -4.563735  ],\n",
       "       [-2.0607045 , -2.9925125 ,  1.304122  , ..., -3.7645273 ,\n",
       "        -3.5458267 , -2.9565072 ],\n",
       "       [ 1.0861759 , -1.0807424 , -2.6614997 , ..., -4.4800687 ,\n",
       "        -2.2402742 , -1.5804557 ]], dtype=float32), label_ids=array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32), metrics={'test_loss': 0.23532433807849884, 'test_f1_samples': 0.2964701574885947, 'test_jaccard': 0.22577692768649743, 'test_hit_rate': 0.5223880597014925, 'test_runtime': 17.2189, 'test_samples_per_second': 66.148, 'test_steps_per_second': 22.069})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_val[\"labels\"] = df_val[\"genres\"].apply(encode_labels)\n",
    "# val_encodings = tokenizer(df_val[\"synopsis\"].tolist(), padding=True, truncation=True, return_tensors=\"np\", max_length=256)\n",
    "# val_dataset = Dataset.from_dict({\n",
    "#     \"input_ids\": val_encodings[\"input_ids\"],\n",
    "#     \"attention_mask\": val_encodings[\"attention_mask\"],\n",
    "#     \"labels\": list(df_val[\"labels\"])\n",
    "# })\n",
    "prediction=trainer.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb335b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_23108\\2529054157.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MultiLabelTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = MultiLabelTrainer(\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 9,194\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,152\n",
      "  Number of trainable parameters = 66,975,004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='1152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   5/1152 00:41 < 4:22:13, 0.07 it/s, Epoch 0.01/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 96\u001b[0m\n\u001b[0;32m     69\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     70\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./anime-genre-model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,         \u001b[38;5;66;03m# Evaluate each epoch\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     85\u001b[0m )\n\u001b[0;32m     87\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MultiLabelTrainer(\n\u001b[0;32m     88\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     89\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     94\u001b[0m )\n\u001b[1;32m---> 96\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\transformers\\trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2558\u001b[0m )\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2566\u001b[0m ):\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\transformers\\trainer.py:3782\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3780\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\accelerate\\accelerator.py:2450\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2450\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2451\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get all unique genres\n",
    "all_genres = sorted(set(genre for sublist in df_train[\"genres\"] for genre in sublist))\n",
    "\n",
    "# Create mappings\n",
    "label2id = {genre: idx for idx, genre in enumerate(all_genres)}\n",
    "id2label = {idx: genre for genre, idx in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "def encode_labels(genres):\n",
    "    vec = np.zeros(num_labels, dtype=np.float32)\n",
    "    for genre in genres:\n",
    "        vec[label2id[genre]] = 1.0\n",
    "    return vec\n",
    "\n",
    "df_train[\"labels\"] = df_train[\"genres\"].apply(encode_labels)\n",
    "df_test[\"labels\"] = df_test[\"genres\"].apply(encode_labels)\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "# Tokenize the text\n",
    "encodings = tokenizer(df_train[\"synopsis\"].tolist(), padding=True, truncation=True, return_tensors=\"np\")\n",
    "encode_test=tokenizer(df_test[\"synopsis\"].tolist(), padding=True, truncation=True, return_tensors=\"np\")\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_ids\": encodings[\"input_ids\"],\n",
    "    \"attention_mask\": encodings[\"attention_mask\"],\n",
    "    \"labels\": list(df_train[\"labels\"])\n",
    "})\n",
    "dataset_test=Dataset.from_dict({\n",
    "    \"input_ids\": encode_test[\"input_ids\"],\n",
    "    \"attention_mask\": encode_test[\"attention_mask\"],\n",
    "    \"labels\": list(df_test[\"labels\"])\n",
    "})\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ckpt,\n",
    "                                    num_labels=num_labels,\n",
    "                                    problem_type=\"multi_label_classification\",\n",
    "                                    id2label=id2label,\n",
    "                                    label2id=label2id)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = BCEWithLogitsLoss()(logits, labels.float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = (pred.predictions > 0.5).astype(int)\n",
    "\n",
    "    f1 = f1_score(labels, preds, average=\"samples\")\n",
    "    jaccard = jaccard_score(labels, preds, average=\"samples\")\n",
    "    hits = (np.logical_and(labels, preds).sum(axis=1) > 0).mean()\n",
    "\n",
    "    return {\"f1_samples\": f1, \"jaccard\": jaccard, \"hit_rate\": hits}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./anime-genre-model\",\n",
    "    eval_strategy=\"epoch\",         # Evaluate each epoch\n",
    "    save_strategy=\"epoch\",               # Save each epoch\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_samples\",\n",
    "    logging_steps=10,                    # Log every 10 steps\n",
    "    logging_dir=\"./logs\",                # Optional: store logs for TensorBoard\n",
    "    report_to=\"none\",                    # or 'tensorboard' if you want\n",
    "    disable_tqdm=False,                  # Show progress bar\n",
    "    log_level=\"info\",                     # Show training logs\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = MultiLabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ac197d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action', 'Adventure', 'Comedy', 'Demons', 'Drama', 'Ecchi', 'Fantasy', 'Harem', 'Hentai', 'Historical', 'Horror', 'Kids', 'Mecha', 'Military', 'Music', 'Mystery', 'Other', 'Parody', 'Romance', 'School', 'Sci-Fi', 'Seinen', 'Shoujo', 'Shounen', 'Slice of Life', 'Sports', 'Super Power', 'Supernatural']\n",
      "Num of unique genres: 28\n",
      "{0: 'Action', 1: 'Adventure', 2: 'Comedy', 3: 'Demons', 4: 'Drama', 5: 'Ecchi', 6: 'Fantasy', 7: 'Harem', 8: 'Hentai', 9: 'Historical', 10: 'Horror', 11: 'Kids', 12: 'Mecha', 13: 'Military', 14: 'Music', 15: 'Mystery', 16: 'Other', 17: 'Parody', 18: 'Romance', 19: 'School', 20: 'Sci-Fi', 21: 'Seinen', 22: 'Shoujo', 23: 'Shounen', 24: 'Slice of Life', 25: 'Sports', 26: 'Super Power', 27: 'Supernatural'}\n",
      "{'Action': 0, 'Adventure': 1, 'Comedy': 2, 'Demons': 3, 'Drama': 4, 'Ecchi': 5, 'Fantasy': 6, 'Harem': 7, 'Hentai': 8, 'Historical': 9, 'Horror': 10, 'Kids': 11, 'Mecha': 12, 'Military': 13, 'Music': 14, 'Mystery': 15, 'Other': 16, 'Parody': 17, 'Romance': 18, 'School': 19, 'Sci-Fi': 20, 'Seinen': 21, 'Shoujo': 22, 'Shounen': 23, 'Slice of Life': 24, 'Sports': 25, 'Super Power': 26, 'Supernatural': 27}\n"
     ]
    }
   ],
   "source": [
    "unique_genres =list(set(genre for sublist in df_train['genres'] for genre in sublist))\n",
    "# Ensure genres are a sorted list\n",
    "unique_genres_list = sorted(list(unique_genres))\n",
    "print(unique_genres_list)\n",
    "print(\"Num of unique genres:\",len(unique_genres))\n",
    "\n",
    "id2label = {k:v for k,v in enumerate(unique_genres_list)}\n",
    "label2id = {v:k for k,v in enumerate(unique_genres_list)}\n",
    "\n",
    "print(id2label)\n",
    "print(label2id)\n",
    "# Your genre-to-index dictionary\n",
    "genre2id = {'Action': 0, 'Adventure': 1, 'Comedy': 2, 'Demons': 3, 'Drama': 4, 'Ecchi': 5,\n",
    "            'Fantasy': 6, 'Harem': 7, 'Hentai': 8, 'Historical': 9, 'Horror': 10, 'Kids': 11,\n",
    "            'Mecha': 12, 'Military': 13, 'Music': 14, 'Mystery': 15, 'Other': 16, 'Parody': 17,\n",
    "            'Romance': 18, 'School': 19, 'Sci-Fi': 20, 'Seinen': 21, 'Shoujo': 22, 'Shounen': 23,\n",
    "            'Slice of Life': 24, 'Sports': 25, 'Super Power': 26, 'Supernatural': 27}\n",
    "\n",
    "num_labels = len(genre2id)\n",
    "\n",
    "def encode_multilabel(example):\n",
    "    vec = np.zeros(num_labels, dtype=np.float32)\n",
    "    for genre_id in example[\"genres\"]:\n",
    "        vec[genre_id] = 1.0\n",
    "    example[\"labels\"] = vec\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f44cb648",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_genres_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Features, ClassLabel, Value, Dataset, DatasetDict ,Sequence\n\u001b[1;32m----> 3\u001b[0m ds_features \u001b[38;5;241m=\u001b[39m Features({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynopsis\u001b[39m\u001b[38;5;124m\"\u001b[39m: Value(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m\"\u001b[39m: Sequence(ClassLabel(names\u001b[38;5;241m=\u001b[39m\u001b[43munique_genres_list\u001b[49m))})\n\u001b[0;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m DatasetDict({\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(df_train\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),features\u001b[38;5;241m=\u001b[39mds_features),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m: Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(df_val\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),features\u001b[38;5;241m=\u001b[39mds_features),\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(df_test\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),features\u001b[38;5;241m=\u001b[39mds_features)})\n\u001b[0;32m     10\u001b[0m dataset\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unique_genres_list' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import Features, ClassLabel, Value, Dataset, DatasetDict ,Sequence\n",
    "\n",
    "ds_features = Features({\"synopsis\": Value(\"string\"), \"genres\": Sequence(ClassLabel(names=unique_genres_list))})\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train.reset_index(drop=True),features=ds_features),\n",
    "    \"valid\": Dataset.from_pandas(df_val.reset_index(drop=True),features=ds_features),\n",
    "    \"test\": Dataset.from_pandas(df_test.reset_index(drop=True),features=ds_features)})\n",
    "\n",
    "dataset\n",
    "dataset_encoded = dataset_encoded.map(encode_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb51b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ad30986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9194/9194 [00:02<00:00, 4106.86 examples/s]\n",
      "Map: 100%|██████████| 1139/1139 [00:00<00:00, 5056.65 examples/s]\n",
      "Map: 100%|██████████| 1127/1127 [00:00<00:00, 5428.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"synopsis\"], padding=True, truncation=True)\n",
    "\n",
    "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2f28221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['synopsis', 'genres', 'input_ids', 'attention_mask']\n",
      "[2, 11]\n",
      "[101, 4076, 1037, 10369, 3005, 2155, 1005, 1055, 3260, 2003, 2000, 8145, 2104, 27578, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] follows a pig whose family ' s mission is to collect underpants. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_encoded[\"train\"].column_names)\n",
    "print(dataset_encoded['train']['genres'][2])\n",
    "print(dataset_encoded['train']['input_ids'][2])\n",
    "decoded_text=tokenizer.decode(dataset_encoded['train']['input_ids'][2])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb8f150b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46581049",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_genres_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification, AutoConfig\n\u001b[1;32m----> 3\u001b[0m num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43munique_genres_list\u001b[49m)\n\u001b[0;32m      5\u001b[0m config \u001b[38;5;241m=\u001b[39m (AutoConfig\n\u001b[0;32m      6\u001b[0m           \u001b[38;5;241m.\u001b[39mfrom_pretrained(model_ckpt, num_labels\u001b[38;5;241m=\u001b[39mnum_labels, \n\u001b[0;32m      7\u001b[0m                            label2id\u001b[38;5;241m=\u001b[39mlabel2id, id2label\u001b[38;5;241m=\u001b[39mid2label))\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m (AutoModelForSequenceClassification\n\u001b[0;32m     10\u001b[0m          \u001b[38;5;241m.\u001b[39mfrom_pretrained(model_ckpt, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m     11\u001b[0m          \u001b[38;5;241m.\u001b[39mto(device))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unique_genres_list' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "num_labels = len(unique_genres_list)\n",
    "\n",
    "config = (AutoConfig\n",
    "          .from_pretrained(model_ckpt, num_labels=num_labels, \n",
    "                           label2id=label2id, id2label=id2label))\n",
    "\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, config=config)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b93d04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, jaccard_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    probs = pred.predictions\n",
    "    preds = (probs > 0.5).astype(int)  # apply sigmoid thresholding\n",
    "\n",
    "    f1 = f1_score(labels, preds, average=\"samples\")\n",
    "    jaccard = jaccard_score(labels, preds, average=\"samples\")\n",
    "    \n",
    "    # Hit rate: at least one correct label per sample\n",
    "    hits = (np.logical_and(labels, preds).sum(axis=1) > 0).mean()\n",
    "\n",
    "    return {\n",
    "        \"f1_samples\": f1,\n",
    "        \"jaccard\": jaccard,\n",
    "        \"hit_rate\": hits\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769c8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62707198",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m----> 4\u001b[0m logging_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdataset_encoded\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_ckpt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(output_dir\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m      7\u001b[0m                                   num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                   learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m                                   save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     18\u001b[0m                                   )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "logging_steps = len(dataset_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{model_ckpt}-finetuned\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=4,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "#                                   push_to_hub=True, \n",
    "                                  log_level=\"error\",\n",
    "                                  save_total_limit=1\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e260b3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3520\\603824525.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MultiLabelTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = MultiLabelTrainer(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 20\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n\u001b[0;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MultiLabelTrainer(\n\u001b[0;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     14\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\transformers\\trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2558\u001b[0m )\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2566\u001b[0m ):\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\transformers\\trainer.py:3736\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3735\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3736\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3738\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3741\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3742\u001b[0m ):\n",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m, in \u001b[0;36mMultiLabelTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# <== added **kwargs\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m      5\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\_collections_abc.py:962\u001b[0m, in \u001b[0;36mMutableMapping.pop\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\u001b[39;00m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;124;03m  If key is not found, d is returned if given, otherwise KeyError is raised.\u001b[39;00m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 962\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__marker:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:271\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03mwith the constraint of slice.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[item]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "class MultiLabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # <== added **kwargs\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss_fct = BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits, labels.float())\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "trainer = MultiLabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75497d36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m----> 3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m, args\u001b[38;5;241m=\u001b[39mtraining_args, \n\u001b[0;32m      4\u001b[0m                   compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m      5\u001b[0m                   train_dataset\u001b[38;5;241m=\u001b[39mdataset_encoded[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      6\u001b[0m                   eval_dataset\u001b[38;5;241m=\u001b[39mdataset_encoded[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      7\u001b[0m                   tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m      8\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, \n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=dataset_encoded[\"train\"],\n",
    "                  eval_dataset=dataset_encoded[\"valid\"],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
